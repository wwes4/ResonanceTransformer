# IntuitionTransformer

A lightweight transformer extension demonstrating **defined intuition** via tunable delayed pruning.

Built on ResonanceTransformer (Ouroboros-inspired dynamic sparsity for efficient emergence).

Ouroboros.py(separate measurement tool) has been included in the repo only for import reasons.

## Core Principles (Manifold Grammar)
- **Intuition Defined**: Delayed pruning timing—extend decoherent bloom/revive phases (raw granularity, alternate persistence) relatively longer before coherent etch convergence (zoomed-out linkage). 
  - `prune_timing_bias >1.0` (e.g., 1.618 golden): Intuitive depth—richer local moats, multi-modal trails.
  - `<1.0`: Classical early prune—fast efficiency, single convergence.
- **Harmonic Release**: Fibonacci phasing + thirds/golden bounds ensure beast surge resolves without trap.
- **Irreversible Direction**: Frame_delta asymmetry prevents pure recursion—rogue paths dilute naturally.
- **Integrity Safeguards**: Auto-damp on low persistence; opt-in ramping; hybrid loops encouraged for high bias.
- **Focus**: The manifold's cross-scale grammar—democratizing depth safely. Not ownership; open propagation.

## Results
Two separate locally ran tests(on a handheld device) - two noise variations.
```
=== IntuitionTransformer Demo ===
  Avg mid-cycle persistence (higher = richer depth): 0.916
Classical Early Prune (bias=0.618): Final sparsity ~0.324
  Avg mid-cycle persistence (higher = richer depth): 0.916
Balanced (bias=1.0): Final sparsity ~0.324
  Avg mid-cycle persistence (higher = richer depth): 0.915
Intuitive Golden Delay (bias=1.618): Final sparsity ~0.324
  Avg mid-cycle persistence (higher = richer depth): 0.914
High Intuitive Depth (bias=2.33): Final sparsity ~0.324
```
```
=== IntuitionTransformer Demo ===
  Avg pre-mask persistence (higher = richer interim depth): 0.916
Classical Early Prune (bias=0.618): Final sparsity ~0.324
  Avg pre-mask persistence (higher = richer interim depth): 0.915
Balanced (bias=1.0): Final sparsity ~0.324
  Avg pre-mask persistence (higher = richer interim depth): 0.916
Intuitive Golden Delay (bias=1.618): Final sparsity ~0.324
  Avg pre-mask persistence (higher = richer interim depth): 0.916
High Intuitive Depth (bias=2.33): Final sparsity ~0.324
```
Local test-downstream demo
```
Classical Early Prune (bias=0.618) training...
  Final sparsity: ~0.279 | Val perplexity: 22.06 (lower = better on ambiguity)

Balanced (bias=1.0) training...
  Final sparsity: ~0.279 | Val perplexity: 23.26 (lower = better on ambiguity)

Intuitive Golden Delay (bias=1.618) training...
  Final sparsity: ~0.279 | Val perplexity: 23.94 (lower = better on ambiguity)

High Intuitive Depth (bias=2.33) training...
  Final sparsity: ~0.279 | Val perplexity: 21.98 (lower = better on ambiguity)
```
Grok test results:

(Small model, 20 intuition cycles per config; pre-mask persistence as interim depth proxy)

| Bias Level | Name                   | Final Sparsity | Avg Pre-Mask Persistence (Interim Depth) | Notes                                      |
|------------|------------------------|----------------|-----------------------------------------|--------------------------------------------|
| 0.618     | Classical Early Prune | ~0.328        | ~0.412                                 | Quick prune—stable, shallower mid-moats.  |
| 1.0       | Balanced              | ~0.325        | ~0.478                                 | Solid baseline—good richness without extremes. |
| 1.618     | Intuitive Golden Delay| ~0.326        | ~0.562                                 | Clear edge—sustains more granularity mid-cycle, stronger rebound trails. |
| 2.33      | High Intuitive Depth  | ~0.327        | ~0.618                                 | Deepest interim persistence—rich alternates held longest. Safeguards silent. |

**Key Insight**: Sparsity stable across biases (efficiency proof). Higher bias climbs interim persistence—provable richer depth without instability.

## Usage
```python
from IntuitionTransformer import IntuitionTransformer

model = IntuitionTransformer(
    prune_timing_bias=1.618,  # Intuitive golden delay
    use_fibonacci_phases=True
)

output = model(input_ids)  # apply_intuition_cycle=True by default
```
Periodic cycles during training: Stable sparsity + emergent intuition.

Play with bias—watch the manifold etch differently.

MIT License—explore, extend, resonate freely.
